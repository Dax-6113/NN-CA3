# -*- coding: utf-8 -*-
"""NN_CA3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/157LIFbZ9K4mPN3W_ie_u2P_09FmZcabt
"""

# Import necessary libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize the Decision Tree classifier with limited depth
decision_tree = DecisionTreeClassifier(max_depth=4, random_state=42)
decision_tree.fit(X_train, y_train)
y_pred_tree = decision_tree.predict(X_test)

# Evaluate the Decision Tree
accuracy_tree = accuracy_score(y_test, y_pred_tree)
report_tree = classification_report(y_test, y_pred_tree)
print(f"Decision Tree Accuracy: {accuracy_tree}")
print("Decision Tree Classification Report:")
print(report_tree)

# Perform cross-validation to get a better sense of accuracy
cv_scores_tree = cross_val_score(decision_tree, X, y, cv=5)
print(f"Decision Tree Cross-Validation Accuracy: {cv_scores_tree.mean()}")

# Initialize the Random Forest classifier with limited depth
random_forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
random_forest.fit(X_train, y_train)
y_pred_forest = random_forest.predict(X_test)

# Evaluate the Random Forest
accuracy_forest = accuracy_score(y_test, y_pred_forest)
report_forest = classification_report(y_test, y_pred_forest)
print(f"Random Forest Accuracy: {accuracy_forest}")
print("Random Forest Classification Report:")
print(report_forest)

# Perform cross-validation to get a better sense of accuracy
cv_scores_forest = cross_val_score(random_forest, X, y, cv=5)
print(f"Random Forest Cross-Validation Accuracy: {cv_scores_forest.mean()}")

# Visualize the Decision Tree
plt.figure(figsize=(20, 10))
plot_tree(decision_tree, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()

# ---- Comparison Section ----
print("\n--- Model Comparison ---")

# Accuracy comparison
print(f"Decision Tree Accuracy: {accuracy_tree}")
print(f"Random Forest Accuracy: {accuracy_forest}")

# Cross-validation comparison
print(f"Decision Tree Cross-Validation Accuracy: {cv_scores_tree.mean()}")
print(f"Random Forest Cross-Validation Accuracy: {cv_scores_forest.mean()}")

# Compare overfitting (Train vs Test Accuracy)
train_accuracy_tree = decision_tree.score(X_train, y_train)
train_accuracy_forest = random_forest.score(X_train, y_train)
print(f"\nDecision Tree Training Accuracy: {train_accuracy_tree}")
print(f"Random Forest Training Accuracy: {train_accuracy_forest}")

# Overfitting behavior
if train_accuracy_tree - accuracy_tree > train_accuracy_forest - accuracy_forest:
    print("\nThe Decision Tree shows more overfitting compared to the Random Forest.")
else:
    print("\nThe Random Forest handles overfitting better than the Decision Tree.")

# Interpretability comment
print("\nInterpretability:")
print("The Decision Tree is more interpretable, as you can visualize the entire decision path.")
print("The Random Forest, being an ensemble of many trees, is harder to interpret as it aggregates decisions from multiple models.")